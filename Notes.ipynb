{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe for MiniLibriSpeech\n",
    "\n",
    "Source:\n",
    "\n",
    "* https://groups.google.com/forum/#!topic/kaldi-help/tzyCwt7zgMQ\n",
    "\n",
    "* https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6\n",
    "\n",
    "* https://eleanorchodroff.com/tutorial/kaldi/training-overview.html\n",
    "\n",
    "* https://jrmeyer.github.io/asr/2016/12/15/DNN-AM-Kaldi.html\n",
    "\n",
    "* https://kaldi-asr.org/doc/kaldi_for_dummies.html\n",
    "\n",
    "* https://kaldi-asr.org/doc/tutorial_running.html for commands to view results and models\n",
    "\n",
    "Recipe taken from crim Kaldi repo cloned with:\n",
    "\n",
    "* `git clone https://www.crim.ca/stash/scm/reco/crim_kaldi_egs.git`\n",
    "\n",
    "It is located at:\n",
    "\n",
    "* `crim_kaldi_egs/mini_librispeech/s5`\n",
    "\n",
    "Snippets of code are taken from:\n",
    "\n",
    "* `crim_kaldi_egs/mini_librispeech/s5/run.sh`\n",
    "\n",
    "## 0. Prepare directory structure and symbolic links\n",
    "\n",
    "### 0.1 Create symbolic links in `crim_kaldi_egs/mini_librispeech/s5` for:\n",
    "\n",
    "* `steps`: `ln -s ../wsj/s5/steps .`\n",
    "    \n",
    "* `utils`: `ln -s ../wsj/s5/utils .`\n",
    "    \n",
    "### 0.2 Create directories if not already present in recipe:\n",
    "\n",
    "* `conf`: Configuration file for specific recipe. The directory `conf`local requires one file mfcc.conf, which contains the parameters for MFCC feature extraction.\n",
    "    \n",
    "* `local`: Local contains data for this specific recipe or project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain a written transcript of the speech data\n",
    "\n",
    "For a more precise alignment, utterance (~sentence) level start and end times are helpful, but not necessary.\n",
    "\n",
    "### 1.1 Consider these variables from now on:\n",
    "\n",
    "```bash\n",
    "# Change this location to somewhere where you want to put the data.\n",
    "data=./corpus/\n",
    "\n",
    "data_url=www.openslr.org/resources/31\n",
    "lm_url=www.openslr.org/resources/11\n",
    "```\n",
    "\n",
    "### 1.2 Consider this command file definition (`cmd.sh`) if you run locally:\n",
    "\n",
    "```bash\n",
    "# you can change cmd.sh depending on what type of queue you are using.\n",
    "# If you have no queueing system and want to run on a local machine, you\n",
    "# can change all instances 'queue.pl' to run.pl (but be careful and run\n",
    "# commands one by one: most recipes will exhaust the memory on your\n",
    "# machine).  queue.pl works with GridEngine (qsub).  slurm.pl works\n",
    "# with slurm.  Different queues are configured differently, with different\n",
    "# queue names and different ways of specifying things like memory;\n",
    "# to account for these differences you can create and edit the file\n",
    "# conf/queue.conf to match your queue's configuration.  Search for\n",
    "# conf/queue.conf in http://kaldi-asr.org/doc/queue.html for more information,\n",
    "# or search for the string 'default_config' in utils/queue.pl or utils/slurm.pl.\n",
    "\n",
    "export train_cmd=\"run.pl\"\n",
    "export decode_cmd=\"run.pl\"\n",
    "export mkgraph_cmd=\"run.pl\"\n",
    "```\n",
    "\n",
    "### 1.3 Run path.sh\n",
    "\n",
    "`path.sh` should work if you have access to the build `/misc/scratch01/reco/osterrfr/kaldi_hg_builds/build_2019-03-24_33_1ac8c922cbf6b2c34756d4b467cfa6067a6dba90`. Otherwise, change the first line of the file with the root of a cloned Kaldi repo: \n",
    "\n",
    "```bash\n",
    "export KALDI_ROOT=`pwd`/../../..\n",
    "```\n",
    "\n",
    "### 1.4 Download datasets\n",
    "\n",
    "```bash\n",
    "# Download dev and test sets\n",
    "# Saved in ./corpus\n",
    "#\n",
    "# name: Mini LibriSpeech ASR corpus\n",
    "# summary: Subset of LibriSpeech corpus for purpose of regression testing\n",
    "# category: speech\n",
    "# license: CC BY 4.0\n",
    "# file: dev-clean-2.tar.gz   development set, \"clean\" speech\n",
    "# file: train-clean-5.tar.gz test set, \"clean\" speech\n",
    "# file: md5sum.txt           md5 checksums of files\n",
    "\n",
    "for part in dev-clean-2 train-clean-5; do\n",
    "  local/download_and_untar.sh $data $data_url $part\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format transcripts for Kaldi\n",
    "\n",
    "Kaldi requires various formats of the transcripts for acoustic model training. You’ll need the start and end times of each utterance, the speaker ID of each utterance, and a list of all words and phonemes present in the transcript.\n",
    "\n",
    "### 2.1 Get language models\n",
    "\n",
    "* **librispeech-lm-corpus.tgz**: 14500 public domain books, used as training material for the LibriSpeech's LM\n",
    "* **librispeech-lm-norm.txt.gz**: Normalized LM training text\n",
    "* **librispeech-vocab.txt**: 200K word vocabulary for the LM\n",
    "* **librispeech-lexicon.txt**: Pronunciations, some of which G2P auto-generated, for all words in the vocabulary\n",
    "* **3-gram.arpa.gz**: 3-gram ARPA LM, not pruned\n",
    "* **3-gram.pruned.1e-7.arpa.gz**: 3-gram ARPA LM, pruned with theshold 1e-7\n",
    "* **3-gram.pruned.3e-7.arpa.gz**: 3-gram ARPA LM, pruned with theshold 3e-7\n",
    "* **4-gram.arpa.gz**: 4-gram ARPA LM, usually used for rescoring\n",
    "* **g2p-model-5**: Fifth order Sequitur G2P model\n",
    "\n",
    "```bash\n",
    "# Download language models\n",
    "# Saved in ./corpus with symlink in data/local/lm\n",
    "#\n",
    "# name: LibriSpeech language models, vocabulary and G2P models\n",
    "# summary: Language modelling resources, for use with the LibriSpeech ASR corpus\n",
    "# category: text\n",
    "# license: Public domain\n",
    "\n",
    "local/download_lm.sh $lm_url $data data/local/lm\n",
    "```\n",
    "\n",
    "### 2.2 Prepare data\n",
    "\n",
    "For dev and train sets from minilibrispeech, prepare the data. Output files are generated in `data/dev_clean_2` and `data/train_clean_5`. These files are:\n",
    "\n",
    "* wav.scp\n",
    "* text\n",
    "* utt2spk\n",
    "* spk2gender\n",
    "* utt2dur\n",
    "\n",
    "```bash\n",
    "  for part in dev-clean-2 train-clean-5; do\n",
    "    # Use underscore-separated names in data directories.\n",
    "    # Returns files in in data/dev_clean_2 and data/train_clean_5:\n",
    "    #     wav.scp, text, utt2spk, spk2gender, utt2dur\n",
    "    # Usage: $0 <src-dir> <dst-dir>\n",
    "    # e.g.: $0 /export/a15/vpanayotov/data/LibriSpeech/dev-clean data/dev-clean\n",
    "    # local/data_prep.sh $data/LibriSpeech/$part data/$(echo $part | sed s/-/_/g)\n",
    "  done\n",
    "```\n",
    "\n",
    "### 2.3 Build lang directory\n",
    "\n",
    "The next script prepares a directory such as `data/lang/`, in the standard format, given a source directory containing a dictionary `lexicon.txt` in a form like:\n",
    "\n",
    "> word phone1 phone2 ... phoneN\n",
    "\n",
    "per line (alternate prons would be separate lines), or a dictionary with probabilities called `lexiconp.txt` in a form:\n",
    "\n",
    "> word pron-prob phone1 phone2 ... phoneN\n",
    "\n",
    "(with 0.0 < pron-prob <= 1.0); note: if `lexiconp.txt` exists, we use it even if `lexicon.txt` exists.\n",
    "\n",
    "Also files `silence_phones.txt`, `nonsilence_phones.tx`t, `optional_silence.txt` and `extra_questions.txt`. Here, `silence_phones.txt` and `nonsilence_phones.txt` are lists of silence and non-silence phones respectively (where silence includes various kinds of noise, laugh, cough, filled pauses etc., and nonsilence phones includes the \"real\" phones.) In each line of those files is a list of phones, and the phones on each line are assumed to correspond to the same \"base phone\", i.e. they will be different stress or tone variations of the same basic phone.\n",
    "\n",
    "The file `optional_silence.txt` contains just a single phone (typically SIL) which is used for optional silence in the lexicon.\n",
    "\n",
    "`extra_questions.txt` might be empty; typically will consist of lists of phones, all members of each list with the same stress or tone; and also possibly a list for the silence phones.  This will augment the automatically generated questions (note: the automatically generated ones will treat all the stress/tone versions of a phone the same, so will not \"get to ask\" about stress or tone).\n",
    "\n",
    "This script adds word-position-dependent phones and constructs a host of other derived files, that go in data/lang/.\n",
    "\n",
    "See http://kaldi-asr.org/doc/data_prep.html#data_prep_lang_creating for more info.\n",
    "\n",
    "```bash\n",
    "  # Usage: utils/prepare_lang.sh <dict-src-dir> <oov-dict-entry> <tmp-dir> <lang-dir>\n",
    "  # e.g.: utils/prepare_lang.sh data/local/dict <SPOKEN_NOISE> data/local/lang data/lang\n",
    "  utils/prepare_lang.sh data/local/dict_nosp \\\n",
    "    \"<UNK>\" data/local/lang_tmp_nosp data/lang_nosp\n",
    "```\n",
    "\n",
    "The script creates `$tmpdir/phone_map.txt` and this has the format (on each line):\n",
    "\n",
    "> <original phone> <version 1 of original phone> <version 2> ...\n",
    "\n",
    "Where the versions depend on the position of the phone within a word. For instance, we'd have:\n",
    "\n",
    "> AA AA_B AA_E AA_I AA_S\n",
    "\n",
    "for (B)egin, (E)nd, (I)nternal and (S)ingleton and in the case of silence:\n",
    "\n",
    "> SIL SIL SIL_B SIL_E SIL_I SIL_S\n",
    "\n",
    "because SIL on its own is one of the variants; this is for when it doesn't occur inside a word but as an option in the lexicon.\n",
    "\n",
    "#### 2.4 Prepare Language Models\n",
    "\n",
    "```bash\n",
    "# Prepare the test time language model(G) transducers\n",
    "# Usage: $0 <lm-dir>\n",
    "# e.g.: $0 /export/a15/vpanayotov/data/lm\n",
    "local/format_lms.sh --src-dir data/lang_nosp data/local/lm\n",
    "\n",
    "# Create ConstArpaLm format language model for full 3-gram and 4-gram LMs\n",
    "# This script reads in an Arpa format language model, and converts it into the\n",
    "# ConstArpaLm format language model.\n",
    "# Usage: \n",
    "#   $0 [options] <arpa-lm-path> <old-lang-dir> <new-lang-dir>\n",
    "utils/build_const_arpa_lm.sh data/local/lm/lm_tglarge.arpa.gz \\\n",
    "  data/lang_nosp data/lang_nosp_test_tglarge\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract acoustic features from the audio\n",
    "\n",
    "Mel Frequency Cepstral Coefficients (MFCC) are the most commonly used features, but Perceptual Linear Prediction (PLP) features and other features are also an option. These features serve as the basis for the acoustic models.\n",
    "\n",
    "### 3.1 Spread MFCCs to different machines\n",
    "\n",
    "Spread the mfccs over various machines, as this data-set is quite large. This script creates storage directories on different file systems, and creates symbolic links to those directories. For example, a command\n",
    "\n",
    "```bash\n",
    "utils/create_split_dir.pl /export/gpu-0{3,4,5}/egs/storage egs/storage\n",
    "```\n",
    "\n",
    "will mkdir -p all of those directories, and will create links\n",
    "\n",
    "* `egs/storage/1` -> `/export/gpu-03/egs/storage`\n",
    "* `egs/storage/2` -> `/export/gpu-03/egs/storage`\n",
    "* ...\n",
    "\n",
    "```bash\n",
    "  # Usage: utils/create_split_dir.pl <actual_storage_dirs> <pseudo_storage_dir>\n",
    "  # e.g.: utils/create_split_dir.pl /export/gpu-0{3,4,5}/egs/storage egs/storage\n",
    "  if [[  $(hostname -f) ==  *.clsp.jhu.edu ]]; then\n",
    "    mfcc=$(basename mfccdir) # in case was absolute pathname (unlikely), get basename.\n",
    "    utils/create_split_dir.pl /export/b{07,14,16,17}/$USER/kaldi-data/egs/librispeech/s5/$mfcc/storage \\\n",
    "      $mfccdir/storage\n",
    "  fi\n",
    "```\n",
    "\n",
    "### 3.2 Compute MFCCs and CMVN stats\n",
    "\n",
    "**MFCC**: Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum.\n",
    "\n",
    "**CMVN**: Cepstral mean and variance normalization (CMVN) is a computationally efficient normalization technique for robust speech recognition. The performance of CMVN is known to degrade for short utterances. This is due to insufficient data for parameter estimation and loss of discriminable information as all utterances are forced to have zero mean and unit variance.\n",
    "\n",
    "```bash\n",
    "  for part in dev_clean_2 train_clean_5; do\n",
    "    # \"Usage: $0 [options] <data-dir> [<log-dir> [<mfcc-dir>] ]\";\n",
    "    # \"e.g.: $0 data/train exp/make_mfcc/train mfcc\"\n",
    "    # \"Note: <log-dir> defaults to <data-dir>/log, and <mfccdir> defaults to <data-dir>/data\"\n",
    "    steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 10 data/$part exp/make_mfcc/$part $mfccdir\n",
    "    \n",
    "    # Compute cepstral mean and variance statistics per speaker.\n",
    "    # We do this in just one job; it's fast.\n",
    "    # This script takes no options.\n",
    "    # \"Usage: $0 [options] <data-dir> [<log-dir> [<cmvn-dir>] ]\";\n",
    "    # \"e.g.: $0 data/train exp/make_mfcc/train mfcc\"\n",
    "    steps/compute_cmvn_stats.sh data/$part exp/make_mfcc/$part $mfccdir\n",
    "  done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train monophone models\n",
    "\n",
    "A monophone model is an acoustic model that does not include any contextual information about the preceding or following phone. It is used as a building block for the triphone models, which do make use of contextual information.\n",
    "\n",
    "**Note**: from this point forward, we will be assuming a Gaussian Mixture Model/Hidden Markov Model (GMM/HMM) framework. This is in contrast to a deep neural network (DNN) system.\n",
    "\n",
    "### 4.1 Training\n",
    "\n",
    "`Delta+delta-delta`: training computes delta and double-delta features, or dynamic coefficients, to supplement the MFCC features. Delta and delta-delta features are numerical estimates of the first and second order derivatives of the signal (features). As such, the computation is usually performed on a larger window of feature vectors. While a window of two feature vectors would probably work, it would be a very crude approximation (similar to how a delta-difference is a very crude approximation of the derivative). Delta features are computed on the window of the original features; the delta-delta are then computed on the window of the delta-features.\n",
    "\n",
    "`LDA-MLLT`: stands for Linear Discriminant Analysis – Maximum Likelihood Linear Transform. The Linear Discriminant Analysis takes the feature vectors and builds HMM states, but with a reduced feature space for all data. The Maximum Likelihood Linear Transform takes the reduced feature space from the LDA and derives a unique transformation for each speaker. MLLT is therefore a step towards speaker normalization, as it minimizes differences among speakers.\n",
    "\n",
    "`LDA`: In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model.\n",
    "\n",
    "`SAT`: stands for Speaker Adaptive Training. SAT also performs speaker and noise normalization by adapting to each specific speaker with a particular data transform. This results in more homogenous or standardized data, allowing the model to use its parameters on estimating variance due to the phoneme, as opposed to the speaker or recording environment.\n",
    "\n",
    "```bash\n",
    "  # Flat start and monophone training, with delta-delta features.\n",
    "  # This script applies cepstral mean normalization (per speaker).\n",
    "  # \"Usage: steps/train_mono.sh [options] <data-dir> <lang-dir> <exp-dir>\"\n",
    "  # \" e.g.: steps/train_mono.sh data/train.1k data/lang exp/mono\"\n",
    "  steps/train_mono.sh --boost-silence 1.25 --nj 5 --cmd \"$train_cmd\" \\\n",
    "    data/train_500short data/lang_nosp exp/mono\n",
    "```\n",
    "\n",
    "### 4.2 Decoding\n",
    "\n",
    "`mkgraph.sh` creates a fully expanded decoding graph (HCLG) that represents all the language-model, pronunciation dictionary (lexicon), context-dependency, and HMM structure in our model.  The output is a Finite State Transducer (FST) that has word-ids on the output, and pdf-ids on the input (these are indexes that resolve to Gaussian Mixture Models). See `http://kaldi-asr.org/doc/graph_recipe_test.html`.\n",
    "\n",
    "`decode.sh` works on CMN + (delta+delta-delta | LDA+MLLT) features; it works out what type of features you used (assuming it's one of these two). Uses Feature space Maximum Likelihood Linear Regression (fMLLR) transforms, which is a widely used technique for speaker adaptation in HMM-based speech recognition.\n",
    "\n",
    "```bash\n",
    "  (\n",
    "    # \"Usage: utils/mkgraph.sh [options] <lang-dir> <model-dir> <graphdir>\"\n",
    "    # \"e.g.: utils/mkgraph.sh data/lang_test exp/tri1/ exp/tri1/graph\"\n",
    "    utils/mkgraph.sh data/lang_nosp_test_tgsmall \\\n",
    "      exp/mono exp/mono/graph_nosp_tgsmall\n",
    "      \n",
    "    for test in dev_clean_2; do\n",
    "      # Usage: steps/decode.sh [options] <graph-dir> <data-dir> <decode-dir>\"\n",
    "      # ... where <decode-dir> is assumed to be a sub-directory of the directory\"\n",
    "      #  where the model is.\"\n",
    "      # e.g.: steps/decode.sh exp/mono/graph_tgpr data/test_dev93 exp/mono/decode_dev93_tgpr\"\n",
    "      steps/decode.sh --nj 10 --cmd \"$decode_cmd\" exp/mono/graph_nosp_tgsmall \\\n",
    "        data/$test exp/mono/decode_nosp_tgsmall_$test\n",
    "    done\n",
    "  )&\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Align audio with the acoustic models\n",
    "\n",
    "The parameters of the acoustic model are estimated in acoustic training steps; however, the process can be better optimized by cycling through training and alignment phases. This is also known as Viterbi training (related, but more computationally expensive procedures include the Forward-Backward algorithm and Expectation Maximization). By aligning the audio to the reference transcript with the most current acoustic model, additional training algorithms can then use this output to improve or refine the parameters of the model. Therefore, each training step will be followed by an alignment step where the audio and text can be realigned.\n",
    "\n",
    "Alignement techniques: https://montreal-forced-aligner.readthedocs.io/en/latest/alignment_techniques.html\n",
    "\n",
    "### 5.1 Alignement\n",
    "\n",
    "The actual alignment algorithm will always be the same; the different scripts accept different types of acoustic model input. Speaker independent alignment, as it sounds, will exclude speaker-specific information in the alignment process.\n",
    "\n",
    "`fMLLR`: stands for Feature Space Maximum Likelihood Linear Regression. After SAT training, the acoustic model is no longer trained on the original features, but on speaker-normalized features. For alignment, we essentially have to remove the speaker identity from the features by estimating the speaker identity (with the inverse of the fMLLR matrix), then removing it from the model (by multiplying the inverse matrix with the feature vector). These quasi-speaker-independent acoustic models can then be used in the alignment process.\n",
    "\n",
    "Computes training alignments using a model with delta or LDA+MLLT features. If you supply the \"--use-graphs true\" option, it will use the training graphs from the source directory (where the model is).  In this case the number of jobs must match with the source directory. \n",
    "\n",
    "```bash \n",
    "  # \"usage: steps/align_si.sh <data-dir> <lang-dir> <src-dir> <align-dir>\"\n",
    "  # \"e.g.:  steps/align_si.sh data/train data/lang exp/tri1 exp/tri1_ali\"\n",
    "  steps/align_si.sh --boost-silence 1.25 --nj 5 --cmd \"$train_cmd\" \\\n",
    "    data/train_clean_5 data/lang_nosp exp/mono exp/mono_ali_train_clean_5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train triphone models\n",
    "\n",
    "While monophone models simply represent the acoustic parameters of a single phoneme, we know that phonemes will vary considerably depending on their particular context. The triphone models represent a phoneme variant in the context of two other (left and right) phonemes.\n",
    "\n",
    "At this point, we’ll also need to deal with the fact that not all triphone units are present (or will ever be present) in the dataset. There are (# of phonemes)<sup>3</sup> possible triphone models, but only a subset of those will actually occur in the data. Furthermore, the unit must also occur multiple times in the data to gather sufficient statistics for the data. A phonetic decision tree groups these triphones into a smaller amount of acoustically distinct units, thereby reducing the number of parameters and making the problem computationally feasible.\n",
    "\n",
    "### 6.1 Train a first delta + delta-delta triphone system (tri1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a first delta + delta-delta triphone system on all utterances\n",
    "\n",
    "```bash\n",
    "# Usage: steps/train_deltas.sh <num-leaves> <tot-gauss> <data-dir> <lang-dir> <alignment-dir> <exp-dir>\n",
    "# e.g.: steps/train_deltas.sh 2000 10000 data/train_si84_half data/lang exp/mono_ali exp/tri1\n",
    "steps/train_deltas.sh --boost-silence 1.25 --cmd \"$train_cmd\" 2000 10000 data/train_clean_5 data/lang_nosp exp/mono_ali_train_clean_5 exp/tri1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Decode first delta + delta-delta triphone model\n",
    "\n",
    "```bash\n",
    "# decode using the tri1 model\n",
    "utils/mkgraph.sh data/lang_nosp_test_tgsmall exp/tri1 exp/tri1/graph_nosp_tgsmall\n",
    "    \n",
    "for test in dev_clean_2; do\n",
    "  steps/decode.sh --nj 5 --cmd \"$decode_cmd\" exp/tri1/graph_nosp_tgsmall \n",
    "  data/$test exp/tri1/decode_nosp_tgsmall_$test\n",
    "  \n",
    "  # Do language model rescoring of lattices (remove old LM, add new LM)\n",
    "  # Usage: steps/lmrescore.sh [options] <old-lang-dir> <new-lang-dir> <data-dir> \n",
    "  #                                     <input-decode-dir> <output-decode-dir>\n",
    "  steps/lmrescore.sh --cmd \"$decode_cmd\" data/lang_nosp_test_{tgsmall,tgmed} \n",
    "  data/$test exp/tri1/decode_nosp_{tgsmall,tgmed}_$test\n",
    "      \n",
    "  # This script rescores lattices with the ConstArpaLm format language model.\n",
    "  # Does language model rescoring of lattices (remove old LM, add new LM)\n",
    "  # Usage: ... [options] <old-lang-dir> <new-lang-dir> \n",
    "  #                      <data-dir> <input-decode-dir> <output-decode-dir>\n",
    "  steps/lmrescore_const_arpa.sh --cmd \"$decode_cmd\" data/lang_nosp_test_{tgsmall,tglarge} \n",
    "  data/$test exp/tri1/decode_nosp_{tgsmall,tglarge}_$test\n",
    "done\n",
    "```\n",
    "\n",
    "### 6.3 Align audio\n",
    "\n",
    "```bash\n",
    "# Realignement\n",
    "steps/align_si.sh --nj 5 --cmd \"$train_cmd\" data/train_clean_5 data/lang_nosp \n",
    "exp/tri1 exp/tri1_ali_train_clean_5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Re-align audio with the acoustic models & re-train triphone models\n",
    "\n",
    "Repeat steps 5 and 6 with additional triphone training algorithms for more refined models. These typically include delta+delta-delta training, LDA-MLLT, and SAT. The alignment algorithms include speaker independent alignments and FMLLR.\n",
    "\n",
    "**Training Algorithms**: Delta+delta-delta training, LDA-MLLT and SAT.\n",
    "\n",
    "**Alignment Algorithms**: fMLLR.\n",
    "\n",
    "### 7.1 LDA+MLLT system. (tri2b)\n",
    "\n",
    "LDA+MLLT refers to the way we transform the features after computing the MFCCs: we splice across several frames, reduce the dimension (to 40 by default) using Linear Discriminant Analysis), and then later estimate, over multiple iterations, a diagonalizing transform known as MLLT or STC.\n",
    "\n",
    "See http://kaldi-asr.org/doc/transform.html for more explanation.\n",
    "\n",
    "### 7.1.1 Train LDA+MLLT system.\n",
    "\n",
    "```bash\n",
    "  # \"Usage: steps/train_lda_mllt.sh [options] <#leaves> <#gauss> <data> <lang> <alignments> <dir>\"\n",
    "  # \" e.g.: steps/train_lda_mllt.sh 2500 15000 data/train_si84 data/lang exp/tri1_ali_si84 exp/tri2b\"\n",
    "  steps/train_lda_mllt.sh --cmd \"$train_cmd\" \\\n",
    "    --splice-opts \"--left-context=3 --right-context=3\" 2500 15000 \\\n",
    "    data/train_clean_5 data/lang_nosp exp/tri1_ali_train_clean_5 exp/tri2b\n",
    "```\n",
    "\n",
    "#### 7.1.2 Decode using the LDA+MLLT model\n",
    "\n",
    "```bash\n",
    "  # decode using the LDA+MLLT model\n",
    "  (\n",
    "    utils/mkgraph.sh data/lang_nosp_test_tgsmall \\\n",
    "      exp/tri2b exp/tri2b/graph_nosp_tgsmall\n",
    "      \n",
    "    for test in dev_clean_2; do\n",
    "      steps/decode.sh --nj 10 --cmd \"$decode_cmd\" exp/tri2b/graph_nosp_tgsmall \\\n",
    "        data/$test exp/tri2b/decode_nosp_tgsmall_$test\n",
    "        \n",
    "      steps/lmrescore.sh --cmd \"$decode_cmd\" data/lang_nosp_test_{tgsmall,tgmed} \\\n",
    "        data/$test exp/tri2b/decode_nosp_{tgsmall,tgmed}_$test\n",
    "        \n",
    "      steps/lmrescore_const_arpa.sh \\\n",
    "        --cmd \"$decode_cmd\" data/lang_nosp_test_{tgsmall,tglarge} \\\n",
    "        data/$test exp/tri2b/decode_nosp_{tgsmall,tglarge}_$test\n",
    "    done\n",
    "  )&\n",
    "```\n",
    "\n",
    "#### 7.1.3 Align audio\n",
    "\n",
    "```bash\n",
    "  # Realign utts using the tri2b model\n",
    "  steps/align_si.sh  --nj 5 --cmd \"$train_cmd\" --use-graphs true \\\n",
    "    data/train_clean_5 data/lang_nosp exp/tri2b exp/tri2b_ali_train_clean_5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 LDA+MLLT+SAT system. (tri3b)\n",
    "\n",
    "This does Speaker Adapted Training (SAT), i.e. train on fMLLR-adapted features.  It can be done on top of either LDA+MLLT, or delta and delta-delta features.  If there are no transforms supplied in the alignment directory, it will estimate transforms itself before building the tree (and in any case, it estimates transforms a number of times during training).\n",
    "\n",
    "#### 7.2.1 Train LDA+MLLT+SAT system.\n",
    "\n",
    "```bash\n",
    "  # \"Usage: steps/train_sat.sh <#leaves> <#gauss> <data> <lang> <ali-dir> <exp-dir>\"\n",
    "  # \" e.g.: steps/train_sat.sh 2500 15000 data/train_si84 data/lang exp/tri2b_ali_si84 exp/tri3b\"\n",
    "  steps/train_sat.sh --cmd \"$train_cmd\" 2500 15000 \\\n",
    "    data/train_clean_5 data/lang_nosp exp/tri2b_ali_train_clean_5 exp/tri3b\n",
    "```\n",
    "\n",
    "#### 7.2.2 Decode using the LDA+MLLT+SAT model \n",
    "\n",
    "Decoding script that does fMLLR. This can be on top of delta+delta-delta, or LDA+MLLT features.\n",
    "\n",
    "There are 3 models involved potentially in this script, and for a standard, speaker-independent system they will all be the same. \n",
    "\n",
    "* The \"alignment model\" is for the 1st-pass decoding and to get the Gaussian-level alignments for the \"adaptation model\" the first time we do fMLLR.  \n",
    "\n",
    "* The \"adaptation model\" is used to estimate fMLLR transforms and to generate state-level lattices.  \n",
    "\n",
    "* The lattices are then rescored with the \"final model\".\n",
    "\n",
    "The following table explains where we get these 3 models from. ($srcdir is one level up from the decoding directory.)\n",
    "\n",
    "| Model              | Default Source                                               |                   |\n",
    "|--------------------|--------------------------------------------------------------|-------------------|\n",
    "| \"alignment model\"  | `$srcdir/final.alimdl (or $srcdir/final.mdl if alimdl absent)` | --alignment-model |\n",
    "| \"adaptation model\" | `$srcdir/final.mdl`                                            | --adapt-model     |\n",
    "| \"final model\"      | `$srcdir/final.mdl`                                            | --final-model     |\n",
    "\n",
    "```bash\n",
    "  # decode using the tri3b model\n",
    "  (\n",
    "    utils/mkgraph.sh data/lang_nosp_test_tgsmall \\\n",
    "      exp/tri3b exp/tri3b/graph_nosp_tgsmall\n",
    "      \n",
    "    for test in dev_clean_2; do\n",
    "      # \"Usage: steps/decode_fmllr.sh [options] <graph-dir> <data-dir> <decode-dir>\"\n",
    "      # \" e.g.: steps/decode_fmllr.sh exp/tri2b/graph_tgpr data/test_dev93 exp/tri2b/decode_dev93_tgpr\"\n",
    "      steps/decode_fmllr.sh --nj 10 --cmd \"$decode_cmd\" \\\n",
    "        exp/tri3b/graph_nosp_tgsmall data/$test \\\n",
    "        exp/tri3b/decode_nosp_tgsmall_$test\n",
    "        \n",
    "      steps/lmrescore.sh --cmd \"$decode_cmd\" data/lang_nosp_test_{tgsmall,tgmed} \\\n",
    "        data/$test exp/tri3b/decode_nosp_{tgsmall,tgmed}_$test\n",
    "        \n",
    "      steps/lmrescore_const_arpa.sh \\\n",
    "        --cmd \"$decode_cmd\" data/lang_nosp_test_{tgsmall,tglarge} \\\n",
    "        data/$test exp/tri3b/decode_nosp_{tgsmall,tglarge}_$test\n",
    "    done\n",
    "  )&\n",
    "```\n",
    "\n",
    "#### 7.2.3 Compute pronunciation and silence probabilities + Recreate Lang Directory\n",
    "\n",
    "Now we compute the pronunciation and silence probabilities from training data, and re-create the lang directory.\n",
    "\n",
    "`get_prons`: This script writes files prons.*.gz in the directory provided, which must contain alignments (ali.*.gz) or lattices (lat.*.gz). These files are as output by nbest-to-prons (see its usage message). As the usage message of nbest-to-prons says, its output has lines that can be interpreted as\n",
    "\n",
    "> `<utterance-id> <begin-frame> <num-frames> <word> <phone1> <phone2> ... <phoneN>`\n",
    "\n",
    "and you could convert these into text form using a command like:\n",
    "\n",
    "```bash\n",
    "gunzip -c prons.*.gz | utils/sym2int.pl -f 4 words.txt | utils/sym2int.pl -f 5- phones.txt\n",
    "```\n",
    "\n",
    "The main steps of this script are:\n",
    "\n",
    "1. Here we figure the count of silence before and after words (actually prons). Create a text like file, but instead of putting words, we write \"word pron\" pairs. We change the format of prons.*.gz from pron-per-line to utterance-per-line (with \"word pron\" pairs tab-separated), and add `<s> and </s>` at the begin and end of each sentence. The _B, _I, _S, _E markers are removed from phones.\n",
    "\n",
    "2. Collect bigram counts for words. To be more specific, we are actually collecting counts for \"v ? w\", where \"?\" represents silence or non-silence.\n",
    "\n",
    "3. Collect bigram counts for silence and words. the count file has 4 fields for counts, followed by the \"word pron\" pair. All fields are separated by spaces:\n",
    "\n",
    "> `<sil-before-count> <nonsil-before-count> <sil-after-count> <nonsil-after-count> <word> <phone1> <phone2 >...`\n",
    "\n",
    "`dict_dir_add_pronprobs.sh`: This script takes pronunciation counts, e.g. generated by aligning your training data and getting the prons using steps/get_prons.sh, and creates a modified dictionary directory with pronunciation probabilities. If the [input-sil-counts] parameter is provided, it will also include silprobs in the generated lexicon.\n",
    "\n",
    "The thing that this script implements is described in the paper:\n",
    "\"PRONUNCIATION AND SILENCE PROBABILITY MODELING FOR ASR\" by Guoguo Chen et al, see http://www.danielpovey.com/files/2015_interspeech_silprob.pdf\n",
    "\n",
    "Create `$dir/lexiconp_silprob.txt` and `$dir/silprob.txt` if silence counts file exists. The format of `$dir/lexiconp_silprob.txt` is:\n",
    "\n",
    "> `word pron-prob P(s_r | w) F(s_l | w) F(n_l | w) pron`\n",
    "\n",
    "where:  \n",
    "\n",
    "* P(s_r | w) is the probability of silence to the right of the word\n",
    "* F(s_l | w) is a factor which is greater than one if silence to the left of the word is more than averagely probable.\n",
    "* F(n_l | w) is a factor which is greater than one if nonsilence to the left of the word is more than averagely probable.\n",
    "\n",
    "```bash  \n",
    "  # usage: $0 <data-dir> <lang-dir> <dir>\n",
    "  # e.g.:  $0 data/train data/lang exp/tri3\n",
    "  # or:  $0 data/train data/lang exp/tri3/decode_dev\n",
    "  steps/get_prons.sh --cmd \"$train_cmd\" \\\n",
    "    data/train_clean_5 data/lang_nosp exp/tri3b\n",
    "    \n",
    "  # Usage: $0 [options] <input-dict-dir> <input-pron-counts> \\\\\"\n",
    "  #           [input-sil-counts] [input-bigram-counts] <output-dict-dir>\"\n",
    "  #  e.g.: $0 data/local/dict \\\\\"\n",
    "  #           exp/tri3/pron_counts_nowb.txt exp/tri3/sil_counts_nowb.txt \\\\\"\n",
    "  #           exp/tri3/pron_bigram_counts_nowb.txt data/local/dict_prons\"\n",
    "  #  e.g.: $0 data/local/dict \\\\\"\n",
    "  #           exp/tri3/pron_counts_nowb.txt data/local/dict_prons\"\n",
    "  utils/dict_dir_add_pronprobs.sh --max-normalize true \\\n",
    "    data/local/dict_nosp \\\n",
    "    exp/tri3b/pron_counts_nowb.txt exp/tri3b/sil_counts_nowb.txt \\\n",
    "    exp/tri3b/pron_bigram_counts_nowb.txt data/local/dict\n",
    "\n",
    "  utils/prepare_lang.sh data/local/dict \\\n",
    "    \"<UNK>\" data/local/lang_tmp data/lang\n",
    "\n",
    "  local/format_lms.sh --src-dir data/lang data/local/lm\n",
    "\n",
    "  utils/build_const_arpa_lm.sh \\\n",
    "    data/local/lm/lm_tglarge.arpa.gz data/lang data/lang_test_tglarge\n",
    "```\n",
    "\n",
    "#### 7.2.4 Align audio\n",
    "\n",
    "Computes training alignments; assumes features are (LDA+MLLT or delta+delta-delta) + fMLLR (probably with SAT models). It first computes an alignment with the final.alimdl (or the final.mdl if final.alimdl is not present), then does 2 iterations of fMLLR estimation.\n",
    "\n",
    "If you supply the --use-graphs option, it will use the training graphs from the source directory (where the model is). In this case the number of jobs must match the source directory.\n",
    "\n",
    "```bash\n",
    "  # \"usage: steps/align_fmllr.sh <data-dir> <lang-dir> <src-dir> <align-dir>\"\n",
    "  # \"e.g.:  steps/align_fmllr.sh data/train data/lang exp/tri1 exp/tri1_ali\"\n",
    "  steps/align_fmllr.sh --nj 5 --cmd \"$train_cmd\" \\\n",
    "    data/train_clean_5 data/lang exp/tri3b exp/tri3b_ali_train_clean_5\n",
    "```\n",
    "\n",
    "#### 7.2.5 Decode using the LDA+MLLT+SAT model with silence and pronunciation probabilities\n",
    "\n",
    "```bash\n",
    "  # Test the tri3b system with the silprobs and pron-probs.\n",
    "\n",
    "  # decode using the tri3b model\n",
    "  utils/mkgraph.sh data/lang_test_tgsmall \\\n",
    "                   exp/tri3b exp/tri3b/graph_tgsmall\n",
    "                   \n",
    "  for test in dev_clean_2; do\n",
    "    steps/decode_fmllr.sh --nj 10 --cmd \"$decode_cmd\" \\\n",
    "                          exp/tri3b/graph_tgsmall data/$test \\\n",
    "                          exp/tri3b/decode_tgsmall_$test\n",
    "                          \n",
    "    steps/lmrescore.sh --cmd \"$decode_cmd\" data/lang_test_{tgsmall,tgmed} \\\n",
    "                       data/$test exp/tri3b/decode_{tgsmall,tgmed}_$test\n",
    "                       \n",
    "    steps/lmrescore_const_arpa.sh \\\n",
    "      --cmd \"$decode_cmd\" data/lang_test_{tgsmall,tglarge} \\\n",
    "      data/$test exp/tri3b/decode_{tgsmall,tglarge}_$test\n",
    "  done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DNN Accoustic Model\n",
    "\n",
    "1h is as 1g but a re-tuned model based on resnet-style TDNN-F layers with bypass connections. Below, 1h2 and 1h3 are just reruns of 1h with different --affix options, to give some idea of the run-to-run variation.\n",
    "\n",
    "\n",
    "```bash\n",
    "local/chain/compare_wer.sh --online exp/chain/tdnn1g_sp exp/chain/tdnn1h_sp \n",
    "                           exp/chain/tdnn1h2_sp exp/chain/tdnn1h3_sp\n",
    "```\n",
    "\n",
    "Results:\n",
    "\n",
    "| System                | tdnn1g_sp | tdnn1h_sp | tdnn1h2_sp | tdnn1h3_sp| \n",
    "|-|-|-|-|-|  \n",
    "| WER dev_clean_2 (tgsmall)      | 13.50     | 12.09     | 12.23     | 12.19| \n",
    "| [online:]         | 13.52     | 12.11     | 12.25     | 12.14| \n",
    "| WER dev_clean_2 (tglarge)       | 9.79     |  8.59     |  8.64     |  8.73| \n",
    "| [online:]          | 9.79     |  8.76     |  8.65     |  8.78| \n",
    "| Final train prob        | -0.0460   | -0.0493   | -0.0490   | -0.0493| \n",
    "| Final valid prob        | -0.0892   | -0.0805   | -0.0803   | -0.0813| \n",
    "| Final train prob (xent)   | -1.1739   | -1.1730   | -1.1742   | -1.1749| \n",
    "| Final valid prob (xent)   | -1.4487   | -1.3872   | -1.3857   | -1.3913| \n",
    "| Num-params                 | 6234672  |  5207856  |  5207856  |  5207856| \n",
    "\n",
    "```bash\n",
    "exp/chain/tdnn1g_sp: num-iters=25 nj=2..5 num-params=6.2M dim=40+100->2328 \n",
    "                     combine=-0.056->-0.055 (over 3) \n",
    "                     xent:train/valid[15,24,final]=(-1.50,-1.23,-1.17/-1.73,-1.52,-1.45) \n",
    "                     logprob:train/valid[15,24,final]=(-0.063,-0.051,-0.046/-0.101,-0.094,-0.089)\n",
    "exp/chain/tdnn1h_sp: num-iters=34 nj=2..5 num-params=5.2M dim=40+100->2328 \n",
    "                     combine=-0.049->-0.046 (over 4) \n",
    "                     xent:train/valid[21,33,final]=(-1.50,-1.22,-1.17/-1.66,-1.44,-1.39) \n",
    "                     logprob:train/valid[21,33,final]=(-0.068,-0.055,-0.049/-0.097,-0.088,-0.080)\n",
    "exp/chain/tdnn1h2_sp: num-iters=34 nj=2..5 num-params=5.2M dim=40+100->2328 \n",
    "                      combine=-0.049->-0.046 (over 4) \n",
    "                      xent:train/valid[21,33,final]=(-1.50,-1.22,-1.17/-1.67,-1.43,-1.39) \n",
    "                      logprob:train/valid[21,33,final]=(-0.068,-0.055,-0.049/-0.096,-0.087,-0.080)\n",
    "exp/chain/tdnn1h3_sp: num-iters=34 nj=2..5 num-params=5.2M dim=40+100->2328 \n",
    "                      combine=-0.050->-0.046 (over 4) \n",
    "                      xent:train/valid[21,33,final]=(-1.51,-1.23,-1.17/-1.67,-1.45,-1.39) \n",
    "                      logprob:train/valid[21,33,final]=(-0.068,-0.055,-0.049/-0.097,-0.089,-0.081)\n",
    "```\n",
    "\n",
    "Obtained from:\n",
    "\n",
    "```bash\n",
    "local/chain/run_tdnn.sh --stage 0\n",
    "```\n",
    "\n",
    "### 8.1 Generate i-vectors\n",
    "\n",
    "This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh (and may eventually be called by more scripts).  It contains the common feature preparation and iVector-related parts of the script.  See those scripts for examples of usage.\n",
    "\n",
    "```bash\n",
    "local/nnet3/run_ivector_common.sh --stage $stage \\\n",
    "                                  --train-set $train_set \\\n",
    "                                  --gmm $gmm \\\n",
    "                                  --nnet3-affix \"$nnet3_affix\" || exit 1;\n",
    "```\n",
    "\n",
    "* Although the nnet will be trained by high resolution data, we still have to perturb the normal data to get the alignment _sp stands for speed-perturbed\n",
    "    * Preparing directory for low-resolution speed-perturbed data (for alignment)\n",
    "    * Making MFCC features for low-resolution speed-perturbed data\n",
    "* Aligning with the perturbed low-resolution data\n",
    "* Create high-resolution MFCC features (with 40 cepstra instead of 13)\n",
    "    * Do volume-perturbation on the training data prior to extracting hires features; this helps make trained nnets more invariant to test data volume.\n",
    "* Computing a subset of data to train the diagonal UBM\n",
    "* Computing a PCA transform from the hires data.\n",
    "* Training the diagonal UBM (Use 512 Gaussians in the UBM).\n",
    "* Train the iVector extractor.  Use all of the speed-perturbed data since iVector extractors can be sensitive to the amount of data. The script defaults to an iVector dimension of 100.\n",
    "* We extract iVectors on the speed-perturbed training data after combining short segments, which will be what we train the system on.  With --utts-per-spk-max 2, the script pairs the utterances into twos, and treats each of these pairs as one speaker; this gives more diversity in iVectors.. Note that these are extracted 'online'.\n",
    "    * Note, we don't encode the 'max2' in the name of the ivectordir even though that's the data we extract the ivectors from, as it's still going to be valid for the non-'max2' data, the utterance list is the same.\n",
    "    * Having a larger number of speakers is helpful for generalization, and to handle per-utterance decoding well (iVector starts at zero).\n",
    "    \n",
    "### 8.2 Create lang directory with chain-type topology\n",
    "\n",
    "Create a version of the lang/ directory that has one state per phone in the topo file. [note, it really has two states.. the first one is only repeated once, the second one has zero or more repeats.]\n",
    "\n",
    "Generate a topology file. This allows control of the number of states in the non-silence HMMs, and in the silence HMMs. This is a modified version of 'utils/gen_topo.pl' that generates a different type of topology, one that we believe should be useful in the 'chain' model.  \n",
    "\n",
    "> Note: right now it doesn't have any real options, and it treats silence and nonsilence the same.  The intention is that you write different versions of this script, or add options, if you experiment with it.\n",
    "\n",
    "```bash\n",
    "cp -r data/lang $lang\n",
    "silphonelist=$(cat $lang/phones/silence.csl) || exit 1;\n",
    "nonsilphonelist=$(cat $lang/phones/nonsilence.csl) || exit 1;\n",
    "# Use our special topology... note that later on may have to tune this\n",
    "# topology.\n",
    "# Usage: steps/nnet3/chain/gen_topo.py\n",
    "# <colon-separated-nonsilence-phones> <colon-separated-silence-phones>\n",
    "# e.g.:  steps/nnet3/chain/gen_topo.pl 4:5:6:7:8:9:10 1:2:3\n",
    "steps/nnet3/chain/gen_topo.py $nonsilphonelist $silphonelist >$lang/topo\n",
    "```    \n",
    "**lang/topo**: We make the transition-probs 0.5 so they normalize, to keep the code happy. In fact, we always set the transition probability scale to 0.0 in the 'chain' code, so they are never used. \n",
    "\n",
    "* Note: the <ForwardPdfClass> will actually happen on the incoming arc because we always build the graph with \"reorder=true\".\n",
    "    \n",
    "### 8.3 Generate alignments\n",
    "\n",
    "Get the alignments as lattices (gives the chain training more freedom). Use the same num-jobs as the alignments.\n",
    "\n",
    "Version of `align_fmllr_lats.sh` that uses \"basis fMLLR\", so it is suitable for situations where there is very little data per speaker (e.g. when there is a one-to-one mapping between utterances and speakers).  Intended for use where the model was trained with basis-fMLLR (i.e.  when you trained the model with train_sat_basis.sh where you normally would have trained with train_sat.sh), or when it was trained with SAT but you ran get_fmllr_basis.sh on the source-model directory.\n",
    "    \n",
    "```bash\n",
    "# usage: steps/align_fmllr_lats.sh <data-dir> <lang-dir> <src-dir> <align-dir>\n",
    "# e.g.:  steps/align_fmllr_lats.sh data/train data/lang exp/tri1 exp/tri1_lats\n",
    "steps/align_fmllr_lats.sh --nj 75 --cmd \"$train_cmd\" ${lores_train_data_dir} \\\n",
    "  data/lang $gmm_dir $lat_dir\n",
    "rm $lat_dir/fsts.*.gz # save space\n",
    "```\n",
    "    \n",
    "### 8.4 Tree Building    \n",
    "     \n",
    "Build a tree using our new topology.  We know we have alignments for the speed-perturbed data (local/nnet3/run_ivector_common.sh made them), so use those.  The num-leaves is always somewhat less than the num-leaves from the GMM baseline.\n",
    "\n",
    "This script builds a tree for use in the 'chain' systems (although the script itself is pretty generic and doesn't use any 'chain' binaries).  This is just like the first stages of a standard system, like 'train_sat.sh', except it does 'convert-ali' to convert alignments to a monophone topology just created from the 'lang' directory (in case the topology is different from where you got the system's alignments from), and it stops after the tree-building and model-initialization stage, without re-estimating the Gaussians or training the transitions.\n",
    "\n",
    "```bash\n",
    "# Usage: $0 <#leaves> <data> <lang> <ali-dir> <exp-dir>\n",
    "# e.g.: $0 --frame-subsampling-factor 3 \n",
    "steps/nnet3/chain/build_tree.sh \\\n",
    "  --frame-subsampling-factor 3 \\\n",
    "  --context-opts \"--context-width=2 --central-position=1\" \\\n",
    "  --cmd \"$train_cmd\" 3500 ${lores_train_data_dir} \\\n",
    "  $lang $ali_dir $tree_dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Creating Neural Net Configs\n",
    "\n",
    "Please note that it is important to have input layer with the name=input as the layer immediately preceding the fixed-affine-layer to enable the use of short notation for the descriptor.\n",
    "\n",
    "```bash\n",
    "num_targets=$(tree-info $tree_dir/tree |grep num-pdfs|awk '{print $2}') \n",
    "learning_rate_factor=$(echo \"print (0.5/$xent_regularize)\" | python)\n",
    "```\n",
    "```bash\n",
    "tdnn_opts=\"l2-regularize=0.03 dropout-proportion=0.0 dropout-per-dim-continuous=true\"\n",
    "tdnnf_opts=\"l2-regularize=0.03 dropout-proportion=0.0 bypass-scale=0.66\"\n",
    "linear_opts=\"l2-regularize=0.03 orthonormal-constraint=-1.0\"\n",
    "prefinal_opts=\"l2-regularize=0.03\"\n",
    "output_opts=\"l2-regularize=0.015\"\n",
    "\n",
    "mkdir -p $dir/configs\n",
    "cat <<EOF > $dir/configs/network.xconfig\n",
    "input dim=100 name=ivector\n",
    "input dim=40 name=input\n",
    "\n",
    "# please note that it is important to have input layer with the name=input\n",
    "# as the layer immediately preceding the fixed-affine-layer to enable\n",
    "# the use of short notation for the descriptor\n",
    "fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat\n",
    "\n",
    "# the first splicing is moved before the lda layer, so no splicing here\n",
    "relu-batchnorm-dropout-layer name=tdnn1 $tdnn_opts dim=768\n",
    "tdnnf-layer name=tdnnf2 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=1\n",
    "tdnnf-layer name=tdnnf3 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=1\n",
    "tdnnf-layer name=tdnnf4 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=1\n",
    "tdnnf-layer name=tdnnf5 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=0\n",
    "tdnnf-layer name=tdnnf6 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf7 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf8 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf9 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf10 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf11 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf12 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "tdnnf-layer name=tdnnf13 $tdnnf_opts dim=768 bottleneck-dim=96 time-stride=3\n",
    "linear-component name=prefinal-l dim=192 $linear_opts\n",
    "\n",
    "## adding the layers for chain branch\n",
    "prefinal-layer name=prefinal-chain input=prefinal-l $prefinal_opts small-dim=192 big-dim=768\n",
    "output-layer name=output include-log-softmax=false dim=$num_targets $output_opts\n",
    "\n",
    "# adding the layers for xent branch\n",
    "prefinal-layer name=prefinal-xent input=prefinal-l $prefinal_opts small-dim=192 big-dim=768\n",
    "output-layer name=output-xent dim=$num_targets learning-rate-factor=$learning_rate_factor $output_opts\n",
    "EOF\n",
    "steps/nnet3/xconfig_to_configs.py --xconfig-file $dir/configs/network.xconfig --config-dir $dir/configs/\n",
    "```\n",
    "\n",
    "### 8.6 Train Neural Net\n",
    "\n",
    "```bash\n",
    "steps/nnet3/chain/train.py --stage=$train_stage \\\n",
    "--cmd=\"$decode_cmd\" \\\n",
    "--feat.online-ivector-dir=$train_ivector_dir \\\n",
    "--feat.cmvn-opts=\"--norm-means=false --norm-vars=false\" \\\n",
    "--chain.xent-regularize $xent_regularize \\\n",
    "--chain.leaky-hmm-coefficient=0.1 \\\n",
    "--chain.l2-regularize=0.0 \\\n",
    "--chain.apply-deriv-weights=false \\\n",
    "--chain.lm-opts=\"--num-extra-lm-states=2000\" \\\n",
    "--trainer.dropout-schedule $dropout_schedule \\\n",
    "--trainer.add-option=\"--optimization.memory-compression-level=2\" \\\n",
    "--trainer.srand=$srand \\\n",
    "--trainer.max-param-change=2.0 \\\n",
    "--trainer.num-epochs=20 \\\n",
    "--trainer.frames-per-iter=3000000 \\\n",
    "--trainer.optimization.num-jobs-initial=2 \\\n",
    "--trainer.optimization.num-jobs-final=5 \\\n",
    "--trainer.optimization.initial-effective-lrate=0.002 \\\n",
    "--trainer.optimization.final-effective-lrate=0.0002 \\\n",
    "--trainer.num-chunk-per-minibatch=128,64 \\\n",
    "--egs.chunk-width=$chunk_width \\\n",
    "--egs.dir=\"$common_egs_dir\" \\\n",
    "--egs.opts=\"--frames-overlap-per-eg 0\" \\\n",
    "--cleanup.remove-egs=$remove_egs \\\n",
    "--use-gpu=true \\\n",
    "--reporting.email=\"$reporting_email\" \\\n",
    "--feat-dir=$train_data_dir \\\n",
    "--tree-dir=$tree_dir \\\n",
    "--lat-dir=$lat_dir \\\n",
    "--dir=$dir  || exit 1;\n",
    "```\n",
    "\n",
    "### 8.7 Make Neural Net Graph\n",
    "\n",
    "```bash\n",
    "# Note: it's not important to give mkgraph.sh the lang directory with the\n",
    "# matched topology (since it gets the topology file from the model).\n",
    "utils/mkgraph.sh \\\n",
    "  --self-loop-scale 1.0 data/lang_test_tgsmall \\\n",
    "  $tree_dir $tree_dir/graph_tgsmall || exit 1;\n",
    "```\n",
    "\n",
    "### 8.8 Decoding\n",
    "\n",
    "```bash\n",
    "for data in $test_sets; do\n",
    "  (\n",
    "    nspk=$(wc -l <data/${data}_hires/spk2utt)\n",
    "    steps/nnet3/decode.sh \\\n",
    "        --acwt 1.0 --post-decode-acwt 10.0 \\\n",
    "        --frames-per-chunk $frames_per_chunk \\\n",
    "        --nj $nspk --cmd \"$decode_cmd\"  --num-threads 4 \\\n",
    "        --online-ivector-dir exp/nnet3${nnet3_affix}/ivectors_${data}_hires \\\n",
    "        $tree_dir/graph_tgsmall data/${data}_hires ${dir}/decode_tgsmall_${data} || exit 1\n",
    "    steps/lmrescore_const_arpa.sh --cmd \"$decode_cmd\" \\\n",
    "      data/lang_test_{tgsmall,tglarge} \\\n",
    "     data/${data}_hires ${dir}/decode_{tgsmall,tglarge}_${data} || exit 1\n",
    "  ) || touch $dir/.error &\n",
    "done\n",
    "```\n",
    "\n",
    "### 8.9 Online Decoding\n",
    "\n",
    "```bash\n",
    "for data in $test_sets; do\n",
    "  (\n",
    "    nspk=$(wc -l <data/${data}_hires/spk2utt)\n",
    "    # note: we just give it \"data/${data}\" as it only uses the wav.scp, the\n",
    "    # feature type does not matter.\n",
    "    steps/online/nnet3/decode.sh \\\n",
    "      --acwt 1.0 --post-decode-acwt 10.0 \\\n",
    "      --nj $nspk --cmd \"$decode_cmd\" \\\n",
    "      $tree_dir/graph_tgsmall data/${data} ${dir}_online/decode_tgsmall_${data} || exit 1\n",
    "    steps/lmrescore_const_arpa.sh --cmd \"$decode_cmd\" \\\n",
    "      data/lang_test_{tgsmall,tglarge} \\\n",
    "     data/${data}_hires ${dir}_online/decode_{tgsmall,tglarge}_${data} || exit 1\n",
    "  ) || touch $dir/.error &\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Training GMM-HMM Acoustic Models\n",
    "\n",
    "https://eleanorchodroff.com/tutorial/kaldi/training-acoustic-models.html\n",
    "\n",
    "### Create files for `data/train`\n",
    "\n",
    "1) Run append_transcripts.py to obtain `text` utterance-to-utterance transcripts file.\n",
    "\n",
    "```bash\n",
    "cd mycorpus/data/train\n",
    "python append_transcripts.py ../local/LibriSpeech_train/train-clean-5/\n",
    "```\n",
    "\n",
    "2) Run the following command to reduce the lexicon to only the words present in the corpus and obtain `words.txt`\n",
    "\n",
    "```bash\n",
    "cd mycorpus/data/train\n",
    "cut -d ' ' -f 2- text | tr ' ' '\\n' | sort -u > words.txt\n",
    "```\n",
    "\n",
    "3) Run the following command to downsize the lexicon to only the words in the corpus tp obtain `/local/lang/lexicon.txt`.\n",
    "\n",
    "```bash\n",
    "cd mycorpus/data/train\n",
    "python filter_dict.py ../lang/lexicon.txt ../local/lang/lexicon.txt words.txt\n",
    "```\n",
    "\n",
    "4) Run the following command to convert flac format to wav format.\n",
    "\n",
    "```bash\n",
    "cd mycorpus/data/train\n",
    "python get_wavs.py ../local/LibriSpeech_train/train-clean-5/ ../local/LibriSpeech_train/train-clean-5-wav/\n",
    "```\n",
    "\n",
    "5) Run the following command to create `segments` file.\n",
    "\n",
    "```bash\n",
    "cd mycorpus\n",
    "python data/train/create_segments.py data/local/LibriSpeech_train/train-clean-5-wav/\n",
    "```\n",
    "\n",
    "6) Run the following command to create `wav.scp` file.\n",
    "\n",
    "```bash\n",
    "cd mycorpus\n",
    "python data/train/create_wav_file.py data/local/LibriSpeech_train/train-clean-5-wav/\n",
    "```\n",
    "\n",
    "7) Run the following command to create `utt2spk` file.\n",
    "\n",
    "```bash\n",
    "cd mycorpus/data/train\n",
    "cat segments | cut -f 1 -d ' ' | \\\n",
    "perl -ane 'chomp; @F = split \"-\", $_; print $_ . \" \" . @F[0] . \"\\n\";' > utt2spk\n",
    "```\n",
    "\n",
    "8) Run the following command to create `spk2utt` file. It will strip `segments` and `wav.scp` from all their files.\n",
    "\n",
    "```bash\n",
    "cd mycorpus\n",
    "utils/fix_data_dir.sh data/train/\n",
    "```\n",
    "### Create files for `data/local/lang`\n",
    "\n",
    "```bash\n",
    "cd ../local/lang\n",
    "```\n",
    "\n",
    "1) Create `nonsilence_phones.txt`.\n",
    "\n",
    "```bash\n",
    "# this should be interpreted as one line of code\n",
    "cut -d ' ' -f 2- lexicon.txt |  \\  \n",
    "tr ' ' '\\n' | \\  \n",
    "sort -u > nonsilence_phones.txt\n",
    "```\n",
    "\n",
    "2) Create `silence_phones.txt`.\n",
    "\n",
    "```bash\n",
    "printf 'SIL\\noov\\n' > silence_phones.txt\n",
    "```\n",
    "\n",
    "3) Create `optional_silence.txt`.\n",
    "\n",
    "```bash\n",
    "printf 'SIL\\n' > optional_silence.txt\n",
    "```\n",
    "\n",
    "### Create files for `data/lang`\n",
    "\n",
    "1) Populate the directory\n",
    "\n",
    "```bash\n",
    "cd mycorpus\n",
    "utils/prepare_lang.sh data/local/lang '<oov>' data/local/ data/lang\n",
    "```\n",
    "\n",
    "### Parallelization wrapper\n",
    "\n",
    "1)\n",
    "```bash\n",
    "cd mycorpus  \n",
    "vim cmd.sh \n",
    "\n",
    "# Insert the following text in cmd.sh\n",
    "train_cmd=\"run.pl\"\n",
    "decode_cmd=\"run.pl\"\n",
    "\n",
    "chmod 750 cmd.sh\n",
    "```\n",
    "\n",
    "2) \n",
    "```bash\n",
    "# Create mfcc.conf by opening it in a text editor like vim\n",
    "cd mycorpus/conf\n",
    "vim mfcc.conf\n",
    "\n",
    "# Insert the following text in mfcc.conf                \n",
    "--use-energy=false  \n",
    "--sample-frequency=16000\n",
    "\n",
    "chmod 750 mfcc.conf\n",
    "```\n",
    "3)\n",
    "```bash\n",
    "cd mycorpus\n",
    "steps/make_mfcc.sh --cmd run.pl --nj 16 data/train/ exp/make_mfcc/data/train mfcc\n",
    "steps/compute_cmvn_stats.sh data/train/ exp/make_mfcc/data/train/ mfcc\n",
    "```\n",
    "\n",
    "### Monophone training and alignment\n",
    "\n",
    "1) Take subset of data for monophone training.\n",
    "```bash\n",
    "cd mycorpus\n",
    "utils/subset_data_dir.sh --first data/train 1000 data/train_1k\n",
    "```\n",
    "\n",
    "2) Train monophones.\n",
    "```bash\n",
    "steps/train_mono.sh --boost-silence 1.25 --nj 10 --cmd run.pl data/train_1k data/lang exp/mono_1k\n",
    "```\n",
    "\n",
    "2) Align monophones.\n",
    "```bash\n",
    "steps/align_si.sh --boost-silence 1.25 --nj 16 --cmd run.pl data/train data/lang exp/mono_1k exp/mono_ali || exit 1;\n",
    "```\n",
    "\n",
    "### Triphone training and alignment\n",
    "\n",
    "1) Train delta-based triphones\n",
    "```bash\n",
    "steps/train_deltas.sh --boost-silence 1.25 --cmd run.pl 2000 10000 data/train data/lang exp/mono_ali exp/tri1 || exit 1;\n",
    "```\n",
    "\n",
    "2) Align delta-based triphones.\n",
    "```bash\n",
    "steps/align_si.sh --nj 24 --cmd run.pl data/train data/lang exp/tri1 exp/tri1_ali || exit 1;\n",
    "```\n",
    "\n",
    "3) Train delta + delta-delta triphones.\n",
    "```bash\n",
    "steps/train_deltas.sh --cmd run.pl 2500 15000 data/train data/lang exp/tri1_ali exp/tri2a || exit 1;\n",
    "```\n",
    "\n",
    "4) Align delta + delta-delta triphones.\n",
    "```bash\n",
    "steps/align_si.sh  --nj 24 --cmd run.pl --use-graphs true data/train data/lang exp/tri2a exp/tri2a_ali  || exit 1;\n",
    "```\n",
    "\n",
    "5) Train LDA-MLLT triphones.\n",
    "```bash\n",
    "steps/train_lda_mllt.sh --cmd run.pl 3500 20000 data/train data/lang exp/tri2a_ali exp/tri3a || exit 1;\n",
    "```\n",
    "\n",
    "6) Align LDA-MLLT triphones with FMLLR.\n",
    "```bash\n",
    "steps/align_fmllr.sh --nj 28 --cmd run.pl data/train data/lang exp/tri3a exp/tri3a_ali || exit 1;\n",
    "```\n",
    "\n",
    "7) Align LDA-MLLT triphones with FMLLR.\n",
    "```bash\n",
    "steps/train_sat.sh  --cmd run.pl 4200 40000 data/train data/lang exp/tri3a_ali exp/tri4a || exit 1;\n",
    "```\n",
    "\n",
    "8) Align SAT triphones with FMLLR.\n",
    "```bash\n",
    "steps/align_fmllr.sh  --cmd run.pl data/train data/lang exp/tri4a exp/tri4a_ali || exit 1;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "## Training DNN Acoustic Models\n",
    "\n",
    "https://jrmeyer.github.io/asr/2016/12/15/DNN-AM-Kaldi.html\n",
    "\n",
    "### First Things First: train a GMM system and generate alignments\n",
    "\n",
    "1) Using all the files generated previously.\n",
    "\n",
    "* `data_dir`: mycorpus/data/train where splitJOBN = split28\n",
    "* `lang_dir`: mycorpus/data/lang\n",
    "* `ali_dir`: mycorpus/exp/tri4a_ali\n",
    "* `mfcc_dir`: mycropus/mfcc\n",
    "\n",
    "```bash\n",
    "## DEPENDENCIES FROM GMM-HMM SYSTEM ##\n",
    "\n",
    "# DATA DIR FILES\n",
    "$data_dir/feats.scp\n",
    "$data_dir/splitJOBN                # where JOBN is the total number of JOBs (eg. split4)\n",
    "                   /JOB            # one dir for each JOB, up to JOBN\n",
    "                        /feats.scp\n",
    "\n",
    "\n",
    "# LANGUAGE DIR FILES\n",
    "$lang_dir/topo\n",
    "\n",
    "\n",
    "# ALIGN DIR FILES\n",
    "$ali_dir/ali.JOB.gz                     # for as many JOBs as you ran\n",
    "$ali_dir/final.mdl\n",
    "$ali_dir/tree\n",
    "$ali_dir/num_jobs\n",
    "\n",
    "\n",
    "# MFCC DIR FILES\n",
    "$mfcc_dir/raw_mfcc_train.JOB.{ark,scp}  # for as many JOBs as you ran\n",
    "```\n",
    "\n",
    "### The Main Run Script: `run_nnet2_simple.sh`\n",
    "\n",
    "1) Create the script following the instructions and using the correct paths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
